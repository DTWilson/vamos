---
title: "BMBR"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{BMBR}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(ggplot2)
library(mvtnorm)
library(tree)
library(mgcv)
```

## Supporting material for BMBR application

Here we include some code to genrate figures for our BMBR application around multiple outcomes.

## Utility elicitation

We want a simple illustration of how a tree classifier can be used to find which region of the utility space we're in, conditional on the observed data.

First, make up some samples from a posterior:

```{r}
m0 <- c(0.3, 0.3)
S0 <- matrix(c(0.3^2, 0.02, 0.02, 0.3^2), ncol = 2)

s <- rmvnorm(10^5, mean = m0, sigma = S0)
s <- as.data.frame(s)
```

Now suppose we have a space of utility functions of the form
$$
\mu_1 + b\mu_2 + ab\mu_1\mu_2,
$$
and want to look at different choices of $a$ and $b$.

```{r}
set.seed(198274)

exp_u <- function(u_p, s)
{
  a <- u_p[1]; b <- u_p[2]
  mean(s[,1] + b*s[,2] + a*b*s[,1]*s[,2])
}

u_ps <- expand.grid(a = seq(0,5,0.5),
                    b = seq(0.8, 1.2, 0.02))


u_ps <- data.frame(a = runif(10^3, 0, 5),
                    b = runif(10^3, 0.8, 1.2))

u_ps$E_u <- apply(u_ps, 1, exp_u, s=s)
u_ps$g <- factor(u_ps$E_u > 0.9)

ggplot(u_ps, aes(a,b,colour = g)) + geom_point()

t <- tree(g ~ a + b, data = u_ps)

u_ps$c <- t$where
  
rects <- data.frame(xmin = c(0, 1.87, 1.87, 2.18, 1.87, 2.59, 2.59, 2.59, 2.59, 2.78, 3.2, 3.73),
                    xmax = c(1.87, 2.18, 2.18, 2.59, 2.59, 2.78, 2.78, 3.2, 3.73, 5, 3.73, 5),
                    ymin = c(0.8, 1.14, 1.05, 1.05, 0.8, 1.01, 0.95, 0.87, 0.8, 0.95, 0.87, 0.8),
                    ymax = c(1.2, 1.2, 1.14, 1.2, 1.05, 1.2, 1.01, 0.95, 0.87, 1.2, 0.95, 0.95),
                    g = factor(c(0,1,0,1,0,1,0,0,0,1,1,1)))
 
ggplot(rects, aes(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax, fill=g)) + geom_rect(colour = "grey50") + 
  geom_rect(data = rects[c(1,11),], colour = "black", linewidth = 1) +
  theme_minimal() + ylab("Utility parameter b") + xlab("Utility parameter a") +
  scale_fill_discrete(name="Decision", labels=c("Stop", "Go"))

ggsave("tree_plot.png", height = 3, width = 5 )
```

## Type I error control

For our second illustration, we want to show how we can estimate a maximum type I error rate from simulated data. 

```{r}
est_post_prob <- function(crit, post_samples, a, c_x, c_y, b_y)
{
  # For a set of posterior samples and a decision rule, return
  # the posterior probability of being in the "go" region and 
  # the samples with each one flagged as go or otherwise
  
  #a = 1000; c_x = 6; c_y = 9; b_y = -2

  mu_x <- post_samples[,1]; mu_y <- post_samples[,2]

  # Get weight for value function
  w <- c_x/(c_y - b_y - a*c_x*b_y)

  # Critical region
  s <- (c_x - crit)/c_x

  crit_y <- c_y - s*(c_y - (w*c_y - c_x)/(w + w*a*c_x))

  crit_xy <- ifelse(mu_x > c_x, crit_y,
         ifelse(mu_x > crit, c_y - s*(c_y - (w*c_y - (c_x - (c_x - mu_x)/s))/(w + w*a*(c_x - (c_x - mu_x)/s))),
                Inf))

  post_samples$go <- mu_y > crit_xy

  return(list(mean(post_samples$go), post_samples))
}

get_post_samples <- function(prior_point, m0, S0)
{
  # For a given sample from the prior, simulate some data (assuming known
  # covariance matrix) and then sample from the conjugate posterior.
  
  # sample data
  s_x <- 1; s_y <-1; tau <- 0.005
  n <- 150
  #cov_zs <- tau/(s_x*s_y)
  #Sig <- matrix(c(1, cov_zs, cov_zs, 1), ncol=2)
  Sig <- matrix(c(2*s_x^2/n, 2*tau/n, 2*tau/n, 2*s_y^2/n), ncol=2)
  d <- t(rmvnorm(1, mean = as.numeric(prior_point),
               sigma = Sig))

  # posterior samples
  post <- rmvnorm(10^5,
                  mean = solve(solve(S0) + solve(Sig)) %*% (solve(S0) %*% m0 + solve(Sig) %*% d),
                  sigma = solve(solve(S0) + solve(Sig)))
  post
}
```


```{r}
set.seed(3218476)

# Prior mean and covariance matrix
m0 <- c(0.3, 0.3)
S0 <- matrix(c(0.3^2, 0.02, 0.02, 0.3^2), ncol = 2)

# Sample from the prior
s <- rmvnorm(10^5, mean = m0, sigma = S0)
s <- as.data.frame(s)

# Utility function parameter values
a = 3; c_x = 0.6; c_y = 0.6; b_y = 0
b_null <- 0; b_alt <- 0.3

# Get the points in our sample in the null space - we can do this by applying
# our decision rule because the rule takes the same form as the hypothesis
# boundary
nulls <- est_post_prob(crit = b_null, post_samples = s, a, c_x, c_y, b_y)[[2]]
nulls <- !nulls$go

# Similarly for the alternative
alts <- est_post_prob(crit = b_alt, post_samples = s, a, c_x, c_y, b_y)[[2]]
alts <- alts$go

# Classify all the prior samples by the hypothesis they are in
s$h <- ifelse(nulls, "null",
              ifelse(alts, "alt", "mid"))


crits <- c(0.1) # seq(0.01, 0.29, 0.025)
ptm <- proc.time()
N <- 10^4
r_b <- matrix(rep(0, N*length(crits)), nrow = N)
r_f <- matrix(rep(0, N*length(crits)), nrow = N)
for(i in 1:N){
  # For each sample
  prior_point <- s[i,1:2]
  # Get a sample of the posterior. This involves first simulating some data,
  # then a conjugate update
  post <- get_post_samples(prior_point, m0, S0)
  for(j in 1:length(crits)){
    # For every candidate decision rule, use the posterior samples to get a 
    # posterior probability of exceeding the rule
    p_b <- est_post_prob(crits[j], as.data.frame(post), a=a, c_x, c_y, b_y)[[1]]
    r_b[i, j] <- log(p_b/(1-p_b))
  }
}

rr <- NULL
for(j in 1:length(crits)){
  # For every candidate decision rule, 
  df_b <- data.frame(x = s[1:N, 1],
                     y = s[1:N, 2],
                     h = s[1:N, 3],
                     g = r_b[,j])
  df_b <- df_b[df_b$g < Inf & df_b$g > -Inf,]
  df_b$d <- df_b$g > 0

  fit <- gam(d ~ ti(x) + ti(y) + ti(x,y), family = binomial(), data = df_b)
  #fit <- gam(g ~ ti(x) + ti(y) + ti(x,y), gaussian(link = "identity"), data = df_b)

  #ss <- s[1:N,]
  ss <- df_b
  ss$lp <- predict(fit, newdata = ss)
  ss$p <- 1/(1+exp(-ss$lp))

  ss_n <- ss[ss$h == "null",]
  #ss_n[which.max(ss_n$p),]
  tI_b <- max(ss_n$p)

  ss_a <- ss[ss$h == "alt",]
  #ss_a[which.min(ss_a$p),]
  tII_b <- 1 - min(ss_a$p)
}

ss_sub <- rbind(ss_n[1:500,], ss_n[which.max(ss_n$p),])

cont_grid <- expand.grid(x = seq(min(ss_n$x), max(ss_n$x), length.out = 100),
                         y = seq(min(ss_n$y), max(ss_n$y), length.out = 100))
cont_grid$lp <- predict(fit, newdata = cont_grid)
cont_grid$p <- 1/(1+exp(-cont_grid$lp))

ggplot(ss_sub, aes(x, y)) + geom_point(alpha = 0.3, colour = "red") +
  geom_point(data = ss[ss$h == "alt",][1:500,], alpha = 0.3, colour = "darkgreen") +
  geom_point(data = ss[ss$h == "mid",][1:500,], alpha = 0.3, colour = "orange") +
  geom_point(data = ss_sub[501,], size = 2, colour = "black") + 
  stat_contour(data = cont_grid, aes(z = p)) +
  theme_minimal()

ggplot(ss_n, aes(x = p)) + geom_histogram(fill = "grey50", colour = "black") +
  theme_minimal()

```

Now we want to estimate the upper bound of this distribution. We do that using extreme value theory - the Fisher–Tippett–Gnedenko theorem tells us the asymptotic distribution of block maxima will be GEV type III (because we know its bounded from above) with some location, scale and shape parameters. We can estimate these parameters by generating some block maxima data - we get a big sample from the null, use our model to predict the type I rate at each point, then chunk the sample up into blocks and get the maximum in each. Then use this data to get MEU estimates of the parameters (explore doing this via MCMC later) via the `evd` package:

```{r}
#set.seed(239857632)
library(evd)

# Sample from the prior
s <- rmvnorm(10^7, mean = m0, sigma = 0.1*S0)
s <- as.data.frame(s)

# Get the points in our sample in the null space - we can do this by applying
# our decision rule because the rule takes the same form as the hypothesis
# boundary
nulls <- est_post_prob(crit = b_null, post_samples = s, a, c_x, c_y, b_y)[[2]]
nulls <- nulls[!nulls$go,]

names(nulls)[1:2] <- c("x", "y")
nulls$lp <- predict(fit, newdata = nulls)
nulls$p <- 1/(1+exp(-nulls$lp))

size <- nrow(nulls)
b <- 10000
blocks <- size %/% b
nulls2 <- nulls[1:(b*blocks),]
maxes <- NULL
for(i in 1:blocks){
  ind <- (i-1)*b + 1
  sub <- nulls2[ind:(ind+b-1),]
  maxes <- c(maxes, max(sub$p))
}

#gev_fit <- fgev(maxes)
#gev_fit

# Then our estimate upper limit is location - scale/shape, i.e.

#gev_pars <- gev_fit$estimate
#gev_pars[1] - gev_pars[2]/gev_pars[3]
```

A Bayesian approach would be nice as it would give a posterior distribution of the upper limit, and we could take an upper quantile estimate from that as our point estimate to be super conservative.

```{r}
library(fitdistcp)
qgev_cp(maxes[!is.na(maxes)], p=c(0.001, 0.5, 0.9999))
```

Both approaches here are telling us about the distribution of the sample maxima when the sample is of size $b$. The Bayesian approach in particular is giving the predictive density of a future maximum. So with $b = 10,000$ we are asking - if we had 10,000 trials, what is the largest type I error rate we would see? This is what we are getting the predictive distribution of using the Bayesian approach, so we can take an upper quantile of that predictive distribution to give a conservative estimate. 

A note: in e.g. Best et al 2025 and references therein, people talk about strict type I error control being impossible as a Bayesian - the only way to get it is to discount all prior information. In our case this is true in the sense that if we want to find the maximum type I error we simply maximise over the null, and the prior doesn't come into play at all. But what we are showing here is that we can get pragmatic type I control - e.g. in the above with block size $10^4$, we can conclude with confidence that 99.99% of trials will have a tI rate less than our point value (the upper 0.9999 quantile of the predictive distribution of block maxima). 

What are the benefits of our approach? The principle benefit is that we don't get type I error rates that are actually of no interest because they come from a point in the parameter space which is a priori infeasible. A secondary benefit is that we avoid optimisation and use sampling instead. This might be simpler, and possibly faster, especially since we only need to build a good surrogate model in the space the prior supports as opposed to the full parameter space - so if we need to use a surrogate (certainly true if we can only simulate individual stop/go outcomes rather than probabilities of going) then this approach is better, even if it ended up getting the same answer in terms of max type I error. 
