---
title: "vamos - reseacrh report"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{research_report}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
#library(vamos)
library(ggplot2)
library(patchwork)
```

## Introduction

This report will compile the theory, evaluation and application of methods for the **vamos** (**va**lue-based testing of **m**ultiple **o**utcome**s**) project. The associated package (which contains this report as a vignette) can be found [here](https://github.com/DTWilson/vamos).

## Problem

Consider a clinical trial comparing two treatments with respect to two normally distributed outcomes with standard deviations $\sigma_i, i=1,2$. Denote the two corresponding treatment effects (i.e., the differences between expected outcomes in the control and experimental treatment groups) and their estimates by $\theta_i$ and $\hat{\theta}_i, i = 1,2$, respectively. Our goal is to develop a procedure for choosing a per-arm sample size $n$ and a critical region $\mathcal{C}$ for a hypothesis test, where we will conclude in favour of the experimental treatment if 

$$
(\hat{z}_1 , \hat{z}_2 ) \in \mathcal{C},
$$
where

$$
\hat{z}_i = \frac{\hat{\theta}_i}{\sqrt{2\sigma_i^2/n}}.
$$

The distribution of $(\hat{z}_1, \hat{z}_2)$ will be bivariate normal with variances of 1 and a covariance of

$$
\begin{aligned}
cov(\hat{z}_1, \hat{z}_2) &= cov\left(\frac{\hat{\theta}_1}{\sqrt{2\sigma_1^2/n}}, \frac{\hat{\theta}_2}{\sqrt{2\sigma_2^2/n}}\right) \\
& = \frac{n}{2 \sigma_1 \sigma_2} cov \left(\hat{\theta_1}, \hat{\theta_2} \right) \\
&= \frac{n}{2 \sigma_1 \sigma_2} \frac{2n}{n^2} \tau\\
&= \frac{\tau}{\sigma_1 \sigma_2}
\end{aligned}
$$
where $\tau$ denotes the covariance between an individual's two outcomes. Since the variances are equal to 1, this covariance is also the correlation.

The conditional distribution of $\hat{z_2}$ given $\hat{z_1}$ is normal with mean

$$
E[\hat{z}_2] + \frac{\tau}{\sigma_1 \sigma_2}(\hat{z}_1 - E[\hat{z}_1])
$$
and variance

$$
1 - \left( \frac{\tau}{\sigma_1 \sigma_2}\right)^2.
$$
  
## Multiple and co-primary endpoints

Two extreme approaches to our problem are based on combining univariate testing procedures for the two outcomes. That is, we choose critical values $c_i, i=1,2$ and then conclude in favour of the experiment treatment when either

- $A = \hat{z}_1 > c_1$ **or** $\hat{z}_2 > c_2$ (the case of multiple primary endpoints), or
- $B = \hat{z}_1 > c_1$ **and** $\hat{z}_2 > c_2$ (the case of co-primary endpoints).

In both cases we choose the design variables $(n, c_1, c_2)$ which minimise $n$ subject to some operating characteristic constraints. For multiple endpoints these are

$$
\begin{align}
\max_{z_1, z_2} & ~ Pr(A ~|~ z_1 \leq 0 \cap z_2 \leq 0) \\
\max_{z_1, z_2} & ~ Pr(A^c  ~|~ z_1 \geq z_1^a \cup z_2 \geq z_2^a),
\end{align}
$$

corresponding to conventional type I and II error rates defined with respect to univariate null (0) and alternative ($z_i^a$) hypotheses. 

For the co-primary case we have instead

$$
\begin{align}
\max_{z_1, z_2} & ~ Pr(B ~|~ z_1 \leq 0 \cup z_2 \leq 0) \\
\max_{z_1, z_2} & ~ Pr(B^c  ~|~ z_1 \geq z_1^a \cap z_2 \geq z_2^a),
\end{align}
$$

again corresponding to type I and II error rates. The hypotheses and critical regions for multiple (i) and co-primary (ii) approaches look like this:

```{r, echo = FALSE}
library(ggplot2)
library(patchwork)

df <- expand.grid(z_1 = c(-5, 7),
                  z_2 = c(-5, 7))

p1 <- ggplot(df, aes(z_1, z_2)) + geom_point(alpha = 0) +
  coord_fixed() + 
  # null
  geom_segment(aes(x = -5, y = 0, xend = 0, yend = 0)) +
  geom_segment(aes(x = 0, y = -5, xend = 0, yend = 0)) + 
  
  # alt
  geom_segment(aes(x = -5, y = 3, xend = 3, yend = 3)) +
  geom_segment(aes(x = 3, y = -5, xend = 3, yend = 3)) +
  
  # crit
  geom_segment(aes(x = -5, y = 2, xend = 2, yend = 2), linetype = 2) +
  geom_segment(aes(x = 2, y = -5, xend = 2, yend = 2), linetype = 2) +
  
  annotate("text", x = -2, y = -2, label = expression(H[0])) +
  annotate("text", x = 5, y = 5, label = expression(H[a])) +
  
  xlab(expression(z[1])) + ylab(expression(z[2])) +
  theme_minimal()

p2 <- ggplot(df, aes(z_1, z_2)) + geom_point(alpha = 0) +
  coord_fixed() + 
  # null
  geom_segment(aes(x = 7, y = 0, xend = 0, yend = 0)) +
  geom_segment(aes(x = 0, y = 7, xend = 0, yend = 0)) + 
  
  # alt
  geom_segment(aes(x = 7, y = 3, xend = 3, yend = 3)) +
  geom_segment(aes(x = 3, y = 7, xend = 3, yend = 3)) +
  
  # crit
  geom_segment(aes(x = 7, y = 2, xend = 2, yend = 2), linetype = 2) +
  geom_segment(aes(x = 2, y = 7, xend = 2, yend = 2), linetype = 2) +
  
  annotate("text", x = -2, y = -2, label = expression(H[0])) +
  annotate("text", x = 5, y = 5, label = expression(H[a])) +
  
  xlab(expression(z[1])) + ylab(expression(z[2])) +
  theme_minimal()  
```

```{r, echo = FALSE, fig.height = 5, fig.width = 7, fig.align = "center"}
p1 + p2 + plot_annotation(tag_levels = 'i')
```

## Implementation

```{r}

```

# Value-based hypotheses

For both the multiple and co-primary approaches, the composite hypotheses imply extreme preference relations between the two outcomes. For example, in the co-primary case both $(z_1, z_2) = (0, 0)$ and $(z_1, z_2) = (5, 0)$ are considered to be in the null hypothesis, meaning that we wish to constrain the probability of concluding in favour of the experimental treatment to the same level at both points. Whenever there is some kind of trade-off between the two outcomes, we might expect the latter point to be preferred to the former and the permitted probability of a positive result relaxed correspondingly. 

To allow for these kind of preferences we can change the way we set up our hypotheses. We propose to construct these by first choosing lower and upper limits beyond which trade-offs are not considered, denoted $c_1$ and $c_2$. We then define a value contour which goes through the point $(0, c_2)$  use this to construct our null and alternative hypotheses by shrinking or expanding this contour with respect to the point $(c_1, c_2)$.

The types of contours we consider are of the form

$$
z_1 + wz_2 + a z_1 w z_2 = C.
$$

The parameter $W$ allows for a relative preference for one endpoint over the other, which the parameter $a$ dictates the nature of the trade-off. With $a = 0$ we will have a linear trade-off; as $a$ increases we will approach the sharp hypothesis illustrated in the co-primary case above. For the canonical contour we note that the value will be equal at points $(0, c_2)$ and $(c_1, n_2)$ (for some choice of $n_2$) and so

$$
\begin{align}
w c_2 &=  c_1 + w n_2 + a c_1 w n_2 \\
w &= \frac{c_1}{c_2 - n_2 - a c_1 n_2}.
\end{align}
$$

This contour is then described by

$$
z_2 = \frac{w c_2 - z_1} {w + a w z_1}.
$$

To transform this into a hypothesis, we specific some value $b$ which denotes the shift from $z_1 = 0$ to $z_1 = b$. This gives a shift parameter 

$$
s = \frac{c_1 - b}{b}
$$

such that we transform our variables from $z_i$ to 

$$
z_i' = c_i - \frac{c_i - z_i}{s}.
$$

Thus, within the trade-off region the hypothesis boundary is given by

$$
c_2 - \frac{c_2 - z_2}{s} = \frac{wc_2 - (c_1 - [c_1 - z_1]/s)}{w + a w (c_1 - [c_1 - z_1]/s)},
$$

while the boundaries become

$$
c_1 - \frac{c_1 - z_1}{s} < 0 ~\text{ and }~ c_2 - \frac{c_2 - z_2}{s} < 0 < n_2.
$$

For example, let $c_1 = c_2 = 7$ with $n_y = 0$, $b_{null} = 0$ and $b_{alt} = 2.49$. Consider $a = 0.5, 1, 5, 100$. These produce the following hypotheses:

```{r}
hyp_plot <- function(a, c_x, c_y = c_x, n_y = 0, b_null = 0, b_alt = 2.486475) {
  
  # Grid of points for plotting
  df <- expand.grid(x = seq(-5,15,0.3),
                 y = seq(-5,15,0.3))
  
  # Get weight for value function
  w <- (c_x)/(c_y - n_y - a*c_x*n_y)
  
  # Get the required shift parameters to transform the basic hypothesis (set up
  # assuming n_x = 0) to null and alternatives
  s_n <- (c_x - b_null)/c_x
  s_a <- (c_x - b_alt)/c_x
  
  # Null hypothesis
  x_n <- seq(b_null, c_x, length.out = 100)
  y_n <- c_y - s_n*(c_y - (w*c_y - (c_x - (c_x - x_n)/s_n))/(w + w*a*(c_x - (c_x - x_n)/s_n)))
  
  # Alternative hypothesis
  x_a <- seq(b_alt, c_x, length.out = 100)
  y_a <- c_y - s_a*(c_y - (w*c_y - (c_x - (c_x - x_a)/s_a))/(w + w*a*(c_x - (c_x - x_a)/s_a)))
  
  df <- data.frame(x = c(x_n, x_a),
                   y = c(y_n, y_a),
                   h = rep(c("null", "alt"), each = 100))
  
  ggplot(df, aes(x, y, colour = h)) + geom_line() +
    xlim(c(-5, 15)) + ylim(c(-5, 15)) +
    geom_segment(aes(x = b_null, xend = b_null, y =  15, yend = c_y, colour = "null")) +
    geom_segment(aes(x = c_x, xend = 15, y =  n_y, yend = n_y, colour = "null")) +
    geom_segment(aes(x = b_alt, xend = b_alt, y =  15, yend = c_y, colour = "alt")) +
    geom_segment(aes(x = c_x, xend = 15, y =  y_a[length(y_a)], yend = y_a[length(y_a)], colour = "alt")) +
    scale_color_manual(name = "Hypothesis", breaks = c("null", "alt"), 
                       values = c(2,4), labels = c("N", "A")) +
    xlab(expression(z[1])) + ylab(expression(z[2])) +
    coord_fixed() +
    theme_minimal()
}

p1 <- hyp_plot(0.2, 7); p2 <- hyp_plot(1, 7); p3 <- hyp_plot(5, 7); p4 <- hyp_plot(100, 7)

(p1 + p2) / (p3 + p4) + plot_layout(guides = "collect")
```

Given we are working on the standardised effect scale, the above assumptions implying a symmetry between the two outcomes may well be reasonable. But we can adjust the relative weighting and hypotheses to specify other more general problems, e.g. when we have a superiority and a non-inferiority outcome with the former considered more important than the latter:

```{r}
hyp_plot(a = 1, c_x = 6, c_y = 9, n_y = -2, b_alt = 2)
```
```{r}
hyp_plot(a = 5, c_x = 7, c_y = 7, n_y = 0, b_alt = 2.4)
hyp_plot(a = 5, c_x = 7, c_y = 9, n_y = 0, b_alt = 2.4)
```

A value-based testing procedure then decides in favour of the experimental treatment when

$$
C = \hat{z}_2 > \frac{c - (\hat{z}_1 - s_{crit})}{1 + a(\hat{z}_1 - s_{crit})} + s_{crit} \text{ and } \hat{z}_1 > b_{crit} \text{ and } \hat{z}_2 > b_{crit},
$$

where 

$$
b_{crit} = \frac{s_{crit}}{1 + a (c - s_{crit})} + s_{crit},
$$

for some choice of critical value $s_{crit}$. Our type I and II error rates are then

$$
\begin{align}
\max_{z_1, z_2} & ~ Pr\left(C ~|~ z_2 < \frac{c - z_1}{1 + az_1} \cup z_1 < 0 \cup z_2 < 0 \right) \\
\max_{z_1, z_2} & ~ Pr\left(C^c  ~|~ z_2 > \frac{c - (z_1 - s)}{1 + a(z_1 - s)} + s \cap z_1 > b \cap z_2 > b \right).
\end{align}
$$

Note that we are assuming a large sample setting such that the covariance matrix of the patient outcomes can be estimated with sufficient precision to let us plug the estimates in and then find the smallest $s_{crit}$ which satisfies the type I error constraint, ensuring that we can always control type I error. As usual, the power of the trial will rely on an accurate prior estimate of the covariance matrix.

```{r}
library(mvtnorm)

a <- 5
c <- 7
sigma <- matrix(c(1, 0, 0, 1), ncol = 2)
cor_z <- sigma[1,2]
variance <- sigma[1,1]
alpha_nom <- 0.05
null <- 0
alternative <- 0.3

vamos(null, alternative, sigma, alpha_nom=0.05, beta_nom=0.2, "value", 1000, .a=a, .c_x=c, .c_y=9, .n_y=0)

ocs_value_cp_wrong(172, null, alternative, cor_z, alpha_nom, variance, a, c_x = c, c_y = 9, n_y = 0)
```
