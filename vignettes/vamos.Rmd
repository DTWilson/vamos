---
title: "vamos - reseacrh report"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{research_report}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
#library(vamos)
library(ggplot2)
library(patchwork)
```

## Introduction

This report will compile the theory, evaluation and application of methods for the **vamos** (**va**lue-based testing of **m**ultiple **o**utcome**s**) project. The associated package (which contains this report as a vignette) can be found [here](https://github.com/DTWilson/vamos).

## Problem

Consider a clinical trial comparing two treatments with respect to two normally distributed outcomes with standard deviations $\sigma_i, i=1,2$. Denote the two corresponding treatment effects (i.e., the differences between expected outcomes in the control and experimental treatment groups) and their estimates by $\theta_i$ and $\hat{\theta}_i, i = 1,2$, respectively. Our goal is to develop a procedure for choosing a per-arm sample size $n$ and a critical region $\mathcal{C}$ for a hypothesis test, where we will conclude in favour of the experimental treatment if 

$$
(\hat{z}_1 , \hat{z}_2 ) \in \mathcal{C},
$$
where

$$
\hat{z}_i = \frac{\hat{\theta}_i}{\sqrt{2\sigma_i^2/n}}.
$$

The distribution of $(\hat{z}_1, \hat{z}_2)$ will be bivariate normal with variances of 1 and a covariance of

$$
\begin{aligned}
cov(\hat{z}_1, \hat{z}_2) &= cov\left(\frac{\hat{\theta}_1}{\sqrt{2\sigma_1^2/n}}, \frac{\hat{\theta}_2}{\sqrt{2\sigma_2^2/n}}\right) \\
& = \frac{n}{2 \sigma_1 \sigma_2} cov \left(\hat{\theta_1}, \hat{\theta_2} \right) \\
&= \frac{n}{2 \sigma_1 \sigma_2} \frac{2n}{n^2} \tau\\
&= \frac{\tau}{\sigma_1 \sigma_2}
\end{aligned}
$$
where $\tau$ denotes the covariance between an individual's two outcomes. Since the variances are equal to 1, this covariance is also the correlation.

The conditional distribution of $\hat{z_2}$ given $\hat{z_1}$ is normal with mean

$$
E[\hat{z}_2] + \frac{\tau}{\sigma_1 \sigma_2}(\hat{z}_1 - E[\hat{z}_1])
$$
and variance

$$
1 - \left( \frac{\tau}{\sigma_1 \sigma_2}\right)^2.
$$
  
## Multiple and co-primary endpoints

Two extreme approaches to our problem are based on combining univariate testing procedures for the two outcomes. That is, we choose critical values $c_i, i=1,2$ and then conclude in favour of the experiment treatment when either

- $A = \hat{z}_1 > c_1$ **or** $\hat{z}_2 > c_2$ (the case of multiple primary endpoints), or
- $B = \hat{z}_1 > c_1$ **and** $\hat{z}_2 > c_2$ (the case of co-primary endpoints).

In both cases we choose the design variables $(n, c_1, c_2)$ which minimise $n$ subject to some operating characteristic constraints. For multiple endpoints these are

$$
\begin{align}
\max_{z_1, z_2} & ~ Pr(A ~|~ z_1 \leq 0 \cap z_2 \leq 0) \\
\max_{z_1, z_2} & ~ Pr(A^c  ~|~ z_1 \geq z_1^a \cup z_2 \geq z_2^a),
\end{align}
$$

corresponding to conventional type I and II error rates defined with respect to univariate null (0) and alternative ($z_i^a$) hypotheses. 

For the co-primary case we have instead

$$
\begin{align}
\max_{z_1, z_2} & ~ Pr(B ~|~ z_1 \leq 0 \cup z_2 \leq 0) \\
\max_{z_1, z_2} & ~ Pr(B^c  ~|~ z_1 \geq z_1^a \cap z_2 \geq z_2^a),
\end{align}
$$

again corresponding to type I and II error rates. The hypotheses and critical regions for multiple (i) and co-primary (ii) approaches look like this:

```{r, echo = FALSE}
library(ggplot2)
library(patchwork)

df <- expand.grid(z_1 = c(-5, 7),
                  z_2 = c(-5, 7))

p1 <- ggplot(df, aes(z_1, z_2)) + geom_point(alpha = 0) +
  coord_fixed() + 
  # null
  geom_segment(aes(x = -5, y = 0, xend = 0, yend = 0)) +
  geom_segment(aes(x = 0, y = -5, xend = 0, yend = 0)) + 
  
  # alt
  geom_segment(aes(x = -5, y = 3, xend = 3, yend = 3)) +
  geom_segment(aes(x = 3, y = -5, xend = 3, yend = 3)) +
  
  # crit
  geom_segment(aes(x = -5, y = 2, xend = 2, yend = 2), linetype = 2) +
  geom_segment(aes(x = 2, y = -5, xend = 2, yend = 2), linetype = 2) +
  
  annotate("text", x = -2, y = -2, label = expression(H[0])) +
  annotate("text", x = 5, y = 5, label = expression(H[a])) +
  
  xlab(expression(z[1])) + ylab(expression(z[2])) +
  theme_minimal()

p2 <- ggplot(df, aes(z_1, z_2)) + geom_point(alpha = 0) +
  coord_fixed() + 
  # null
  geom_segment(aes(x = 7, y = 0, xend = 0, yend = 0)) +
  geom_segment(aes(x = 0, y = 7, xend = 0, yend = 0)) + 
  
  # alt
  geom_segment(aes(x = 7, y = 3, xend = 3, yend = 3)) +
  geom_segment(aes(x = 3, y = 7, xend = 3, yend = 3)) +
  
  # crit
  geom_segment(aes(x = 7, y = 2, xend = 2, yend = 2), linetype = 2) +
  geom_segment(aes(x = 2, y = 7, xend = 2, yend = 2), linetype = 2) +
  
  annotate("text", x = -2, y = -2, label = expression(H[0])) +
  annotate("text", x = 5, y = 5, label = expression(H[a])) +
  
  xlab(expression(z[1])) + ylab(expression(z[2])) +
  theme_minimal()  
```

```{r, echo = FALSE, fig.height = 5, fig.width = 7, fig.align = "center"}
p1 + p2 + plot_annotation(tag_levels = 'i')
```

## Implementation

```{r}

```

# Value-based hypotheses

For both the multiple and co-primary approaches, the composite hypotheses imply extreme preference relations between the two outcomes. For example, in the co-primary case both $(z_1, z_2) = (0, 0)$ and $(z_1, z_2) = (5, 0)$ are considered to be in the null hypothesis, meaning that we wish to constrain the probability of concluding in favour of the experimental treatment to the same level at both points. Whenever there is some kind of trade-off between the two outcomes, we might expect the latter point to be preferred to the former and the permitted probability of a positive result relaxed correspondingly. 

To allow for these kind of preferences we can change the way we set up our hypotheses. We propose to construct these by first choosing lower and upper limits beyond which trade-offs are not considered. To begin, we take these limits to be the same for both $z_1$ and $z_2$ (which may well make sense since these are standardised effects). We set the lower limit to be 0 (without loss of generality?) and denote the upper limit by $c$. Then if $z_1 < 0$ we are in the null, regardless of the value of $z_2$ (and vice versa); while if $z_1 > c$ then our position (in the null, alternative, or in between) depends only on $z_2$.

Within this box, we allow for trade-offs. We model the boundary of the null hypothesis using 

$$
z_1 + z_2 + a z_1 z_2 = c.
$$

The choice of positive $a$ will dictate the nature of the trade-off. With $a = 0$ we will have a linear trade-off; as $a$ increases we will approach the sharp hypothesis illustrated in the co-primary case above. The null is then defined by 

$$
H_0: z_2 < \frac{c - z_1}{1 + az_1} \cup z_1 < 0 \cup z_2 < 0.
$$

For the alternative we use a similar boundary but with our coordinates shifted by $s$:

$$
(z_1 - s) + (z_2 - s) + a (z_1 - s) (z_2 - s) = c.
$$

To find $s$, note that we will require this equation to be satisfied at the point $(c, b)$ where $b$ is the alternative hypothesis at the margins when trade-offs don't exist, when the marginal test will have type I and II error rates of $\alpha$ and $\beta$:

$$
b = \Phi^{-1}(1 - \alpha) + \Phi^{-1}(1 - \beta).
$$
Substituting these values in for $z_1$ and $z_2$ and solving for $s$ gives

$$
s = \frac{(2 + ac + ab) - \sqrt{(-2 - ac - ab)^2 - 4 a (b + abc)}}{2a}.
$$

The alternative hypothesis is then defined by

$$
H_a: z_2 > \frac{c - (z_1 - s)}{1 + a(z_1 - s)} + s \cap z_1 > b \cap z_2 > b.
$$

For example, let $c = 7$ and consider $a = 0.5, 1, 5, 100$. These produce the following hypotheses:

```{r}
hyp_plot <- function(a, c, alpha = 0.05, beta = 0.2) {
  
  df <- expand.grid(x = seq(-5,15,0.3),
                 y = seq(-5,15,0.3))
  
  # Start by finding the required dif at the margins, corresponding to the
  # mean of the marginal sampling dist under the alternative. Note we are using
  # z statistics, so they will have sd = 1.
  alpha <- 0.05; beta <- 0.2
  b <- qnorm(1 - alpha) + qnorm(1 - beta)
  
  # Now translate this into the required shift parameter, s
  aa <- a; bb <- (-2 - a*c - b*a); cc <- b + b*a*c
  s <- (-bb - sqrt(bb^2 - 4*aa*cc))/(2*aa)
  
  # At each point in the grid, denote if it lies in the null (0) or alternative (2)
  # hypotheses.
  df$h <- ifelse(df$y < (c - df$x)/(1 + a*df$x) | df$x < 0 | df$y < 0 , 0,
                 ifelse(df$y > (c - (df$x - s))/(1 + a*(df$x - s)) + s & (df$y > b & df$x > b), 2,
                        1))
  
  # Plot the hypotheses - adding a line at x=b which should border the alternative
  # region.
  ggplot(df, aes(x, y, colour=as.factor(h))) + geom_point() + coord_fixed() +
    scale_color_discrete(name = "Hypothesis") +
    xlab(expression(z[1])) + ylab(expression(z[2]))
}

p1 <- hyp_plot(0.2, 7); p2 <- hyp_plot(1, 7); p3 <- hyp_plot(5, 7); p4 <- hyp_plot(100, 7)

(p1 + p2) / (p3 + p4)
```

A value-based testing procedure then decides in favour of the experimental treatment when

$$
C = \hat{z}_2 > \frac{c - (\hat{z}_1 - s_{crit})}{1 + a(\hat{z}_1 - s_{crit})} + s_{crit} \text{ and } \hat{z}_1 > b_{crit} \text{ and } \hat{z}_2 > b_{crit},
$$

where 

$$
b_{crit} = \frac{s_{crit}}{1 + a (c - s_{crit})} + s_{crit},
$$

for some choice of critical value $s_{crit}$. Our type I and II error rates are then

$$
\begin{align}
\max_{z_1, z_2} & ~ Pr\left(C ~|~ z_2 < \frac{c - z_1}{1 + az_1} \cup z_1 < 0 \cup z_2 < 0 \right) \\
\max_{z_1, z_2} & ~ Pr\left(C^c  ~|~ z_2 > \frac{c - (z_1 - s)}{1 + a(z_1 - s)} + s \cap z_1 > b \cap z_2 > b \right).
\end{align}
$$

```{r}
int_cond_z_1 <- function(z_1, mean, cor_z, a, c, b_crit, crit) {
  # For a given z_1, calculate the conditional probability of landing in the
  # critical region based on the conditional distribution z_2 | z_1, when
  # the joint dist of (z_1, z_2) has a given mean and correlation
  
  # Get mean m and sd s of the conditional distribution
  m <- mean[2] + cor_z*(z_1 - mean[1])
  s <- sqrt(1 - cor_z^2)
  
  # Find the lower limit for the integration over z_2
  low <- ifelse(z_1 > c, b_crit, 
                ifelse(z_1 > b_crit, (c - (z_1- crit))/(1 + a*(z_1 - crit)) + crit,
                       Inf))
  
  # Calculate the conditional prob and weight by the marginal 
  # prob of z_1
  (1 - pnorm(low, m, s))*dnorm(z_1, mean[1], 1)
}

value_tI <- function(crit, a, c, cor_z, b_crit) {
  
  # Search over the null hypothesis boundary to find the maximum type I error
  # rate. 
  # Start search at the point where z_1 = z_2
  z_1_min <- (sqrt(a*c + 1) - 1)/a
  z_1 <- seq(z_1_min, 10, 0.1)
  # Find corresponding z_2 on the value-based boundary
  z_2 <- (c - z_1)/(1 + a*z_1)
  # Correct for when we are in the no-trade-off region
  z_2 <- pmax(z_2, 0)
  
  # Run a a simple exhaustive search
  tI <- NULL
  for(i in 1:length(z_1)){
    tI <- c(tI, integrate(int_cond_z_1, -Inf, Inf, 
                          mean = c(z_1[i], z_2[i]), 
                          cor_z=cor_z, a=a, c=c, b_crit=b_crit, crit=crit)$value)
  }
  
  return(max(tI))
}

value_ocs <- function(crit, a, c, sigma) {
  
  # Find the distance from the null of 0 to the critical region
  # at the margins
  b_crit <- crit/(1 + a*(c - crit)) + crit
  
  # Get correlation of z statistics based on the outcome covariance sigma
  cor_z <- sigma[1,2]/(sqrt(sigma[1,1]*sigma[2,2]))
  
  tI <- value_tI(crit, a, c, cor_z, b_crit)
  
  # Find distance of alternative from null at the margins
  b <- qnorm(1 - alpha) + qnorm(1 - beta)
  # Get the shift parameter corresponding to the alternative hypothesis
  aa <- a; bb <- (-2 - a*c - b*a); cc <- b + b*a*c
  sh <- (-bb - sqrt(bb^2 - 4*aa*cc))/(2*aa)
  
  # Start search at the point where z_1 = z_2
  z_1_min <- (sqrt((2 - 2*a*sh)^2 - 4*a*(a*sh^2 - 2*sh - c)) - (2 - 2*a*sh))/(2*a)
  z_1 <- seq(z_1_min, 10, 0.1)
  # Find corresponding z_2 on the value-based boundary
  z_2 <- (c - (z_1 - sh))/(1 + a*(z_1 - sh)) + sh
  # Correct for when we are in the no-trade-off region
  z_2 <- pmax(z_2, b)
  
  # Run a a simple exhaustive search
  tII <- NULL
  for(i in 1:length(z_1)){
    tII <- c(tII, 1 - integrate(int_cond_z_1, -Inf, Inf, 
                                mean = c(z_1[i], z_2[i]), 
                                cor_z=cor_z, a=a, c=c, b_crit=b_crit, crit=crit)$value)
  }
  
  return(c(max(tI), max(tII)))
}

alpha <- 0.05; beta <- 0.2

sigma <- matrix(c(1, 0.8, 0.8, 1), ncol = 2)

value_ocs(1, a = 1, c = 7, sigma = sigma)

value_ocs_cp <- function(crit, a, c, sigma) {
  # Get OCs under the value hypothesis but when using co-primary critical
  # region.
  
  # Get correlation of z statistics based on the outcome covariance sigma
  cor_z <- sigma[1,2]/(sqrt(sigma[1,1]*sigma[2,2]))
  
  # Search over the null hypothesis boundary to find the maximum type I error
  # rate. 
  # Start search at the point where z_1 = z_2
  z_1_min <- (sqrt(a*c + 1) - 1)/a
  z_1 <- seq(z_1_min, 10, 0.1)
  # Find corresponding z_2 on the value-based boundary
  z_2 <- (c - z_1)/(1 + a*z_1)
  # Correct for when we are in the no-trade-off region
  z_2 <- pmax(z_2, 0)
  
  # Run a a simple exhaustive search
  tI <- NULL
  for(i in 1:length(z_1)){
    tI <- c(tI, pmvnorm(lower = c(crit, crit), upper = c(Inf, Inf),
                        mean = c(z_1[i], z_2[i]), sigma = matrix(c(1, cor_z, cor_z, 1), ncol = 2)))
  }
  
   # Find distance of alternative from null at the margins
  b <- qnorm(1 - alpha) + qnorm(1 - beta)
  # Get the shift parameter corresponding to the alternative hypothesis
  aa <- a; bb <- (-2 - a*c - b*a); cc <- b + b*a*c
  sh <- (-bb - sqrt(bb^2 - 4*aa*cc))/(2*aa)
  
  # Start search at the point where z_1 = z_2
  z_1_min <- (sqrt((2 - 2*a*sh)^2 - 4*a*(a*sh^2 - 2*sh - c)) - (2 - 2*a*sh))/(2*a)
  z_1 <- seq(z_1_min, 10, 0.1)
  # Find corresponding z_2 on the value-based boundary
  z_2 <- (c - (z_1 - sh))/(1 + a*(z_1 - sh)) + sh
  # Correct for when we are in the no-trade-off region
  z_2 <- pmax(z_2, b)
  
  # Run a a simple exhaustive search
  tII <- NULL
  for(i in 1:length(z_1)){
    tII <- c(tII, 1 - pmvnorm(lower = c(crit, crit), upper = c(Inf, Inf),
                        mean = c(z_1[i], z_2[i]), sigma = matrix(c(1, cor_z, cor_z, 1), ncol = 2)))
  }
  
  return(c(max(tI), max(tII)))
}
```


```{r}
library(mvtnorm)

a <- 5
c <- 7
sigma <- matrix(c(1, 0.8, 0.8, 1), ncol = 2)

crits1 <- seq(-2, 4, 0.1)
r <- NULL
for(i in crits1) {
  r <- rbind(r, value_ocs(i, a, c, sigma))
}

crits1 <- seq(-1, 5, 0.1)
r2 <- NULL
for(i in crits1) {
  r2 <- rbind(r2, value_ocs_cp(i, a, c, sigma))
}

plot(r)
points(r2, col = "red")
```
