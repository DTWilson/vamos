---
title: "vamos - research report"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{vamos - research report}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(vamos)
library(ggplot2)
library(patchwork)
```

## Introduction

This report will compile the theory, evaluation and application of methods for the **vamos** (**va**lue-based testing of **m**ultiple **o**utcome**s**) project. The associated package (which contains this report as a vignette) can be found [here](https://github.com/DTWilson/vamos).

## Problem

Consider a clinical trial comparing two treatments with respect to two normally distributed outcomes with standard deviations $\sigma_i, i=1,2$. Denote the two corresponding treatment effects (i.e., the differences between expected outcomes in the control and experimental treatment groups) and their estimates by $\theta_i$ and $\hat{\theta}_i, i = 1,2$, respectively. Our goal is to develop a procedure for choosing a per-arm sample size $n$ and a critical region $\mathcal{C}$ for a hypothesis test, where we will conclude in favour of the experimental treatment if 

$$
(\hat{z}_1 , \hat{z}_2 ) \in \mathcal{C},
$$
where

$$
\hat{z}_i = \frac{\hat{\theta}_i}{\sqrt{2\sigma_i^2/n}}.
$$

The distribution of $(\hat{z}_1, \hat{z}_2)$ will be bivariate normal with variances of 1 and a covariance of

$$
\begin{aligned}
cov(\hat{z}_1, \hat{z}_2) &= cov\left(\frac{\hat{\theta}_1}{\sqrt{2\sigma_1^2/n}}, \frac{\hat{\theta}_2}{\sqrt{2\sigma_2^2/n}}\right) \\
& = \frac{n}{2 \sigma_1 \sigma_2} cov \left(\hat{\theta_1}, \hat{\theta_2} \right) \\
&= \frac{n}{2 \sigma_1 \sigma_2} \frac{2n}{n^2} \tau\\
&= \frac{\tau}{\sigma_1 \sigma_2}
\end{aligned}
$$
where $\tau$ denotes the covariance between an individual's two outcomes.

The conditional distribution of $\hat{z_2}$ given $\hat{z_1}$ is normal with mean

$$
E[\hat{z}_2] + \frac{\tau}{\sigma_1 \sigma_2}(\hat{z}_1 - E[\hat{z}_1])
$$
and variance

$$
1 - \left( \frac{\tau}{\sigma_1 \sigma_2}\right)^2.
$$
  
## Multiple and co-primary endpoints

Two extreme approaches to our problem are based on combining univariate testing procedures for the two outcomes. That is, we choose critical values $c_i, i=1,2$ and then conclude in favour of the experiment treatment when either

- $A = \hat{z}_1 > c_1$ **or** $\hat{z}_2 > c_2$ (the case of multiple primary endpoints), or
- $B = \hat{z}_1 > c_1$ **and** $\hat{z}_2 > c_2$ (the case of co-primary endpoints).

In both cases we choose the design variables $(n, c_1, c_2)$ which minimise $n$ subject to some operating characteristic constraints. For multiple endpoints these are

$$
\begin{align}
\max_{z_1, z_2} & ~ Pr(A ~|~ z_1 \leq 0 \cap z_2 \leq 0) \\
\max_{z_1, z_2} & ~ Pr(A^c  ~|~ z_1 \geq z_1^a \cup z_2 \geq z_2^a),
\end{align}
$$

corresponding to conventional type I and II error rates defined with respect to univariate null (0) and alternative ($z_i^a$) hypotheses. 

For the co-primary case we have instead

$$
\begin{align}
\max_{z_1, z_2} & ~ Pr(B ~|~ z_1 \leq 0 \cup z_2 \leq 0) \\
\max_{z_1, z_2} & ~ Pr(B^c  ~|~ z_1 \geq z_1^a \cap z_2 \geq z_2^a),
\end{align}
$$

again corresponding to type I and II error rates. The hypotheses and critical regions for multiple (i) and co-primary (ii) approaches look like this:

```{r, echo = FALSE}
library(ggplot2)
library(patchwork)

df <- expand.grid(z_1 = c(-5, 7),
                  z_2 = c(-5, 7))

p1 <- ggplot(df, aes(z_1, z_2)) + geom_point(alpha = 0) +
  coord_fixed() + 
  # null
  geom_segment(aes(x = -5, y = 0, xend = 0, yend = 0)) +
  geom_segment(aes(x = 0, y = -5, xend = 0, yend = 0)) + 
  
  # alt
  geom_segment(aes(x = -5, y = 3, xend = 3, yend = 3)) +
  geom_segment(aes(x = 3, y = -5, xend = 3, yend = 3)) +
  
  # crit
  geom_segment(aes(x = -5, y = 2, xend = 2, yend = 2), linetype = 2) +
  geom_segment(aes(x = 2, y = -5, xend = 2, yend = 2), linetype = 2) +
  
  annotate("text", x = -2, y = -2, label = "H[0]", parse = TRUE) +
  annotate("text", x = 5, y = 5, label = "H[a]", parse = TRUE) +
  
  xlab(expression(z[1])) + ylab(expression(z[2])) +
  theme_minimal()

p2 <- ggplot(df, aes(z_1, z_2)) + geom_point(alpha = 0) +
  coord_fixed() + 
  # null
  geom_segment(aes(x = 7, y = 0, xend = 0, yend = 0)) +
  geom_segment(aes(x = 0, y = 7, xend = 0, yend = 0)) + 
  
  # alt
  geom_segment(aes(x = 7, y = 3, xend = 3, yend = 3)) +
  geom_segment(aes(x = 3, y = 7, xend = 3, yend = 3)) +
  
  # crit
  geom_segment(aes(x = 7, y = 2, xend = 2, yend = 2), linetype = 2) +
  geom_segment(aes(x = 2, y = 7, xend = 2, yend = 2), linetype = 2) +
  
  annotate("text", x = -2, y = -2, label = "H[0]", parse = TRUE) +
  annotate("text", x = 5, y = 5, label = "H[a]", parse = TRUE) +
  
  xlab(expression(z[1])) + ylab(expression(z[2])) +
  theme_minimal()  
```

```{r, echo = FALSE, fig.height = 5, fig.width = 7, fig.align = "center"}
p1 + p2 + plot_annotation(tag_levels = 'i')
```

Since co-primary and multiple endpoints are not especially descriptive terms, we will henceforth refer to these as conjunctive and disjunctive tests respectively, following Senn2007.

## Value-based hypotheses

For both the conjunctive and disjunctive approaches, the composite hypotheses imply extreme preference relations between the two outcomes. For example, in the conjunctive case both $(z_1, z_2) = (0, 0)$ and $(z_1, z_2) = (5, 0)$ are considered to be in the null hypothesis, meaning that we wish to constrain the probability of concluding in favour of the experimental treatment to the same level at both points. Whenever there is some kind of trade-off between the two outcomes, we might expect the latter point to be preferred to the former and the permitted probability of a positive result relaxed correspondingly. 

To allow for these kind of preferences we can change the way we set up our hypotheses. We propose to construct these by first choosing lower and upper limits beyond which trade-offs are not considered, denoted $c_1$ and $c_2$. We then define a value contour which goes through the point $(0, c_2)$ and use this to construct our null and alternative hypotheses by shrinking or expanding this contour with respect to the point $(c_1, c_2)$.

The types of contours we consider are of the form

$$
z_1 + wz_2 + a z_1 w z_2 = k,
$$

for some constant $k$. The parameter $W$ allows for a relative preference for one endpoint over the other, while the parameter $a$ dictates the nature of the trade-off. With $a = 0$ we will have a linear trade-off; as $a$ increases we will approach the sharp hypothesis illustrated in the conjunctive case above. We will construct hypotheses by first defining the contour which passes through the point $(z_1, z_2) = (0, c_2)$, and then shrink or expand this line with respect to the point $(c_1, c_2)$.

In addition to the parameters $c_1, c_2$ and $a$, the final input parameter we need is denoted $b_2$ and is chosen such that we have the same value at $(0, c_2)$ and $(c_1, b_2)$. Then we can get the weight $w$:

$$
w = c_1/(c_2 - b_2 - ab_2c_1).
$$
Now, since we want $z_2 = 0$ at the point $z_2 = c_2$, we have $k = wc_2$. Our canonical contour is then

$$
z_2 = \frac{w c_2 - z_1} {w + a w z_1}.
$$

To transform this into a hypothesis, we specify some value $b_1$ such that the hypothesis should pass through the point $(b_1, c_2)$. Our hypothesis boundary is then given by mapping the canonical contour to

$$
s \left( \begin{bmatrix} 
z_1 \\ 
z_2 \\ 
\end{bmatrix}  - 
\begin{bmatrix} 
c_1 \\ 
c_2 \\ 
\end{bmatrix}
\right) + 
\begin{bmatrix} 
c_1 \\ 
c_2 \\ 
\end{bmatrix},
$$

where

$$
s = \frac{c_1 - b}{c_1}.
$$

For example, let $c_1 = c_2 = 7$ with $n_y = 0$, $b_{null} = 0$ and $b_{alt} = 2.49$. Consider $a = 0.2, 1, 5, 20$. These produce the following hypotheses:

```{r}
hyp_plot <- function(a, c_x, c_y = c_x, b_y = 0, b_null = 0, b_alt = 2.486475) {
  
  if(a >= 0){
    # co-primary
    x_up <- c_x + 2; y_up <- c_y + 2
    x_lo <- b_null - 2; y_lo <- b_y -2
  } else {
    # mutliple primary
    x_up <- b_alt + 2; y_up <- b_y + 2
    x_lo <- c_x - 2; y_lo <- c_y - 2
  }
  
  # Grid of points for plotting
  df <- expand.grid(x = seq(x_lo, x_up, 0.3),
                 y = seq(y_lo, y_up, 0.3))
  
  # Get weight for value function
  w <- (c_x)/(c_y - b_y - a*c_x*b_y)
  
  # Get the canonical hypothesis passing through (0, c_y)
  df <- data.frame(x = seq(0, c_x, length.out = 100))
  df$y <- (w*c_y - df$x)/(w + a*w*df$x)

  # Null
  s <- (c_x - b_null)/c_x
  df_null <- as.data.frame(t( s*(t(df) - c(c_x, c_y)) + c(c_x, c_y) ))
  
  # Alternative
  s <- (c_x - b_alt)/c_x
  df_alt <- as.data.frame(t( s*(t(df) - c(c_x, c_y)) + c(c_x, c_y) ))
  
  df <- rbind(df_null, df_alt)
  df$h <- rep(c("null", "alt"), each = 100)

  
  ggplot(df, aes(x, y, colour = h)) + geom_line() +
    xlim(c(x_lo, x_up)) + ylim(c(y_lo, y_up)) +
    
    # Straight lines for the null
    geom_segment(aes(x = b_null, xend = b_null, y = (a >= 0)*y_up + (a < 0)*y_lo, yend = c_y, colour = "null")) +
    geom_segment(aes(x = c_x, xend = (a >= 0)*x_up + (a < 0)*x_lo, y =  df_null$y[nrow(df_null)-1], yend = df_null$y[nrow(df_null)-1], colour = "null")) +
      
    # Straight lines for the alternative
    geom_segment(aes(x = b_alt, xend = b_alt, y = (a >= 0)*y_up + (a < 0)*y_lo, yend = c_y, colour = "alt")) +
    geom_segment(aes(x = c_x, xend = (a >= 0)*x_up + (a < 0)*x_lo, y =  df_alt$y[nrow(df_alt)-1], yend = df_alt$y[nrow(df_alt)-1], colour = "alt")) +
    
    scale_color_manual(name = "Hypothesis", breaks = c("null", "alt"), 
                       values = c(2,4), labels = c("N", "A")) +
    xlab(expression(z[1])) + ylab(expression(z[2])) +
    coord_fixed() +
    theme_minimal()
}

p1 <- hyp_plot(0.2, 7); p2 <- hyp_plot(1, 7); p3 <- hyp_plot(5, 7); p4 <- hyp_plot(20, 7)
```

```{r, echo = FALSE, fig.height = 5, fig.width = 7, fig.align = "center"}
(p1 + p2) / (p3 + p4) + plot_layout(guides = "collect")
```

Given we are working on the standardised effect scale, the above assumptions implying a symmetry between the two outcomes may well be reasonable. But we can adjust the relative weighting and hypotheses to specify other more general problems, e.g. when we have a superiority and a non-inferiority outcome with the former considered more important than the latter:

```{r, echo = FALSE, fig.height = 5, fig.width = 7, fig.align = "center"}
hyp_plot(a = 1, c_x = 6, c_y = 9, b_y = -2, b_null = 0, b_alt = 2)
```


We can create similar hypotheses for the case of multiple primary outcomes. For example,

```{r, echo = FALSE, fig.height = 5, fig.width = 7, fig.align = "center"}
hyp_plot(a = -3, c_x = -5, c_y = -5, b_y = -1, b_null = -2, b_alt = 0)
```

In principle, a value-based testing procedure can proceed by using the same procedure as for creating hypothesis boundaries to create a critical region. This critical region will be determined by one parameter, which we could choose to control type I error. But type I error control would need us to know the sampling distribution, which we won't - in particular, we won't know the correlation between the endpoints. 

## A Frequentist analysis 

For now, let's assume we **do** know the correlation and explore a Frequentist analysis based on a hypothesis test with controlled operating characteristics. In fact, we'll assume the full covariance matrix is known.

Here, we give a null and alternative hypothesis by specifying a mean value, and `sigma` is the outcome covariance matrix. In particular, we give the hypotheses with respect to $z_1$ (the x-axis in our plots) in the area where $z_2$ doesn't come into play.

```{r}
v <- vamos(null = 0, alternative = 0.3, sigma = matrix(c(1, 0, 0, 1), nrow = 2), alpha_nom = 0.05, beta_nom = 0.2, a = 1, c_x = 5, n_y = 0, max_n = 1000)

print(v)
plot(v)
```

To double check things are working, we can plot some diagnostics - for both the null and alternative, plot i) the boundary with the point maximising type I (type II) error; and ii) the type I (type II) error as we move along the boundary.

```{r}
check_errors(v)
```

We want to compare this against the alternative of a typical co-primary endpoint test. We can do that by taking the optimal sample size found here and then finding the max type II error rate whilst controlling the type I rate at the same level.

```{r}
ocs_value_cp_wrong(n=v$n, null=0, alternative=0.3, sigma = matrix(c(1, 0, 0, 1), nrow = 2), alpha_nom=0.05, a=1, c_x=5, n_y=0) 
```
We see an increase in the type II rate of more than 50%.

This analysis is helpful in the sense that it shows how using box-type critical regions will be inefficient if the hypotheses have a different shape characterised by a smooth trade-off. But operationally it is not a lot of use since calculating the critical value for the test requires a known covariance matrix for our outcome. 

## A Bayesian analysis

The unknown correlation, along with all the other model parameters, can be estimated. And if we do this in the Bayesian framework then we aren't reliant on pre-specified rules based on sampling distributions - we can use the posterior instead. We can take the same form of decision rule and look at our posterior over it, deciding in favour if we are more likely to be above than below. We can estimate that probability using the Monte Carlo method. As before the decision rule can be parametrised to make it more or less conservative.

Rather then type I and II error rates, we can instead calculate the conditional probabilities of progression under the null and stopping under the alternative; or equivalent sensitivity and specificity statistics. This will set up future work where we want strict error rate control.

```{r}

```

## Results

```{r}
library(mvtnorm)

a <- 2
c <- 7
sigma <- matrix(c(1, 0, 0, 1), ncol = 2)
cor_z <- sigma[1,2]
variance <- sigma[1,1]
alpha_nom <- 0.05
null <- 0.0
alternative <- 0.3

v <- vamos(null, alternative, sigma, alpha_nom, beta_nom=0.2, a=a, c_x=c, c_y=c, n_y=0)
v$n

ocs_value_cp_wrong(151, null, alternative, cor_z, alpha_nom, variance, a, c_x = c, c_y = c, n_y = 0)
```


## Bayesian

A quick note: if we want to go straight to a Bayesian approach, or to include that as an extension, the following code simulates some bivariate normal data and then does a congugate analysis. The main thing is that it shows a correlation on the outcomes translates to a correlation in the posterior means, and thus supports our general argument still - that uncorrelated endpoints will generally give us lower variance when estimating a joint value or utility.

```{r}
library(MASS)

# Parameters
set.seed(123)
n <- 500
mu <- c(3, 5)
Sigma <- matrix(c(2, 0,
                  0, 2), nrow = 2)

# Simulate data
data <- mvrnorm(n, mu = mu, Sigma = Sigma)

# Plot
plot(data, main = "Simulated Bivariate Normal Data",
     xlab = "X1", ylab = "X2", pch = 19, col = rgb(0, 0, 1, 0.5))

kappa0 <- 1
m0 <- c(0, 0)
nu0 <- 4          # must be > dimension - 1 = 1
S0 <- diag(2)     # prior scale matrix


# Sample mean
xbar <- colMeans(data)

# Scatter matrix
S <- t(data - matrix(xbar, nrow=n, ncol=2, byrow=TRUE)) %*% (data - matrix(xbar, nrow=n, ncol=2, byrow=TRUE))

# Posterior updates
kappa_n <- kappa0 + n
m_n <- (kappa0 * m0 + n * xbar) / kappa_n
nu_n <- nu0 + n

# Scale matrix update
delta <- matrix(xbar - m0, ncol=1)
Sn <- S0 + S + (kappa0 * n / kappa_n) * (delta %*% t(delta))

# Posterior mean
print(m_n)

# Posterior scale matrix
print(Sn)

library(MCMCpack)

# Sample Sigma ~ Inverse-Wishart
Sigma_post <- riwish(nu_n, Sn)

# Sample mu | Sigma
mu_post <- mvrnorm(1, mu = m_n, Sigma = Sigma_post / kappa_n)

print(mu_post)
print(Sigma_post)

mu_posts <- mvrnorm(10^3, mu = m_n, Sigma = Sigma_post / kappa_n)
plot(mu_posts)
cov(mu_posts)
cor(mu_posts)
```

## BMBR support

Want a simple illustration of how a tree classifier can be used to find which region of the utility space we're in, conditional on the observed data.

First, make up some samples from a posterior:

```{r}
library(ggplot2)
library(mvtnorm)
library(tree)
library(mgcv)

m0 <- c(0.3, 0.3)
S0 <- matrix(c(0.3^2, 0.02, 0.02, 0.3^2), ncol = 2)

s <- rmvnorm(10^5, mean = m0, sigma = S0)
s <- as.data.frame(s)
```

Now suppose we have a space of utility functions of the form
$$
\mu_1 + b\mu_2 + ab\mu_1\mu_2,
$$
and want to look at different choices of $a$ and $b$.

```{r}
set.seed(198274)

exp_u <- function(u_p, s)
{
  a <- u_p[1]; b <- u_p[2]
  mean(s[,1] + b*s[,2] + a*b*s[,1]*s[,2])
}

u_ps <- expand.grid(a = seq(0,5,0.5),
                    b = seq(0.8, 1.2, 0.02))


u_ps <- data.frame(a = runif(10^3, 0, 5),
                    b = runif(10^3, 0.8, 1.2))

u_ps$E_u <- apply(u_ps, 1, exp_u, s=s)
u_ps$g <- factor(u_ps$E_u > 0.9)

ggplot(u_ps, aes(a,b,colour = g)) + geom_point()

t <- tree(g ~ a + b, data = u_ps)

u_ps$c <- t$where

#ggplot(u_ps, aes(a,b,colour = as.factor(c))) + geom_point()

#rects <- data.frame(xmin = c(0, 1.87, 1.87, 2.18, 1.87, 2.59, 2.59, 2.59, 2.78, 3.2, 3.73),
#                    xmax = c(1.87, 2.18, 2.18, 2.59, 2.59, 2.78))

#ggplot(u_ps, aes(a,b,colour = as.factor(c))) + geom_point()
  
rects <- data.frame(xmin = c(0, 1.87, 1.87, 2.18, 1.87, 2.59, 2.59, 2.59, 2.59, 2.78, 3.2, 3.73),
                    xmax = c(1.87, 2.18, 2.18, 2.59, 2.59, 2.78, 2.78, 3.2, 3.73, 5, 3.73, 5),
                    ymin = c(0.8, 1.14, 1.05, 1.05, 0.8, 1.01, 0.95, 0.87, 0.8, 0.95, 0.87, 0.8),
                    ymax = c(1.2, 1.2, 1.14, 1.2, 1.05, 1.2, 1.01, 0.95, 0.87, 1.2, 0.95, 0.95),
                    g = factor(c(0,1,0,1,0,1,0,0,0,1,1,1)))
 
ggplot(rects, aes(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax, fill=g)) + geom_rect(colour = "grey50") + 
  geom_rect(data = rects[c(1,11),], colour = "black", linewidth = 1) +
  theme_minimal() + ylab("Relative weight, b") + xlab("Interaction, a") +
  scale_fill_discrete(name="Decision", labels=c("Stop", "Go"))
```

For our second illustration, we want to show how we can estimate a maximum type I error rate from simulated data. 



```{r}
est_post_prob <- function(crit, post_samples, a, c_x, c_y, b_y)
{
  # For a set of posterior samples and a decision rule, return
  # the posterior probability of being in the "go" region and 
  # the samples with each one flagged as go or otherwise
  
  #a = 1000; c_x = 6; c_y = 9; b_y = -2

  mu_x <- post_samples[,1]; mu_y <- post_samples[,2]

  # Get weight for value function
  w <- c_x/(c_y - b_y - a*c_x*b_y)

  # Critical region
  s <- (c_x - crit)/c_x

  crit_y <- c_y - s*(c_y - (w*c_y - c_x)/(w + w*a*c_x))

  crit_xy <- ifelse(mu_x > c_x, crit_y,
         ifelse(mu_x > crit, c_y - s*(c_y - (w*c_y - (c_x - (c_x - mu_x)/s))/(w + w*a*(c_x - (c_x - mu_x)/s))),
                Inf))

  post_samples$go <- mu_y > crit_xy

  return(list(mean(post_samples$go), post_samples))
}

get_post_samples <- function(prior_point, m0, S0)
{
  # For a given sample from the prior, simulate some data (assuming known
  # covariance matrix) and then sample from the conjugate posterior.
  
  # sample data
  s_x <- 1; s_y <-1; tau <- 0.005
  n <- 150
  #cov_zs <- tau/(s_x*s_y)
  #Sig <- matrix(c(1, cov_zs, cov_zs, 1), ncol=2)
  Sig <- matrix(c(2*s_x^2/n, 2*tau/n, 2*tau/n, 2*s_y^2/n), ncol=2)
  d <- t(rmvnorm(1, mean = as.numeric(prior_point),
               sigma = Sig))

  # posterior samples
  post <- rmvnorm(10^5,
                  mean = solve(solve(S0) + solve(Sig)) %*% (solve(S0) %*% m0 + solve(Sig) %*% d),
                  sigma = solve(solve(S0) + solve(Sig)))
  post
}
```


```{r}
set.seed(3218476)

# Prior mean and covariance matrix
m0 <- c(0.3, 0.3)
S0 <- matrix(c(0.3^2, 0.02, 0.02, 0.3^2), ncol = 2)

# Sample from the prior
s <- rmvnorm(10^5, mean = m0, sigma = S0)
s <- as.data.frame(s)

# Utility function parameter values
a = 3; c_x = 0.6; c_y = 0.6; b_y = 0
b_null <- 0; b_alt <- 0.3

# Get the points in our sample in the null space - we can do this by applying
# our decision rule because the rule takes the same form as the hypothesis
# boundary
nulls <- est_post_prob(crit = b_null, post_samples = s, a, c_x, c_y, b_y)[[2]]
nulls <- !nulls$go

# Similarly for the alternative
alts <- est_post_prob(crit = b_alt, post_samples = s, a, c_x, c_y, b_y)[[2]]
alts <- alts$go

# Classify all the prior samples by the hypothesis they are in
s$h <- ifelse(nulls, "null",
              ifelse(alts, "alt", "mid"))


crits <- c(0.1) # seq(0.01, 0.29, 0.025)
ptm <- proc.time()
N <- 10^4
r_b <- matrix(rep(0, N*length(crits)), nrow = N)
r_f <- matrix(rep(0, N*length(crits)), nrow = N)
for(i in 1:N){
  # For each sample
  prior_point <- s[i,1:2]
  # Get a sample of the posterior. This involves first simulating some data,
  # then a conjugate update
  post <- get_post_samples(prior_point, m0, S0)
  for(j in 1:length(crits)){
    # For every candidate decision rule, use the posterior samples to get a 
    # posterior probability of exceeding the rule
    p_b <- est_post_prob(crits[j], as.data.frame(post), a=a, c_x, c_y, b_y)[[1]]
    r_b[i, j] <- log(p_b/(1-p_b))
  }
}

rr <- NULL
for(j in 1:length(crits)){
  # For every candidate decision rule, 
  df_b <- data.frame(x = s[1:N, 1],
                     y = s[1:N, 2],
                     h = s[1:N, 3],
                     g = r_b[,j])
  df_b <- df_b[df_b$g < Inf & df_b$g > -Inf,]

  #fit <- gam(g ~ ti(x) + ti(y) + ti(x,y), family = binomial(), data = df_b)
  fit <- gam(g ~ ti(x) + ti(y) + ti(x,y), gaussian(link = "identity"), data = df_b)

  #ss <- s[1:N,]
  ss <- df_b
  ss$lp <- predict(fit, newdata = ss)
  ss$p <- 1/(1+exp(-ss$lp))

  ss_n <- ss[ss$h == "null",]
  #ss_n[which.max(ss_n$p),]
  tI_b <- max(ss_n$p)

  ss_a <- ss[ss$h == "alt",]
  #ss_a[which.min(ss_a$p),]
  tII_b <- 1 - min(ss_a$p)
}

ss_sub <- rbind(ss_n[1:500,], ss_n[which.max(ss_n$p),])

cont_grid <- expand.grid(x = seq(min(ss_n$x), max(ss_n$x), length.out = 100),
                         y = seq(min(ss_n$y), max(ss_n$y), length.out = 100))
cont_grid$lp <- predict(fit, newdata = cont_grid)
cont_grid$p <- 1/(1+exp(-cont_grid$lp))

ggplot(ss_sub, aes(x, y)) + geom_point(alpha = 0.3, colour = "red") +
  geom_point(data = ss[ss$h == "alt",][1:500,], alpha = 0.3, colour = "darkgreen") +
  geom_point(data = ss[ss$h == "mid",][1:500,], alpha = 0.3, colour = "orange") +
  geom_point(data = ss_sub[501,], size = 2, colour = "black") + 
  stat_contour(data = cont_grid, aes(z = p)) +
  theme_minimal()

ggplot(ss_n, aes(x = p)) + geom_histogram(fill = "grey50", colour = "black") +
  theme_minimal()

```

Now we want to estimate the upper bound of this distribution. We do that using extreme value theory - the Fisher–Tippett–Gnedenko theorem tells us the asymptotic distribution of block maxima will be GEV (because we know its bounded from above) with some location, scale and shape parameters. We can estimate these parameters by generating some block maxima data - we get a big sample from the null, use our model to predict the type I rate at each point, then chunk the sample up into blocks of 500 and get the maximum in each. Then use this data to get MEU estimates of the parameters (explore doing this via MCMC later) via the `evd` package:

```{r}
#set.seed(239857632)
library(evd)

# Sample from the prior
s <- rmvnorm(10^6, mean = m0, sigma = S0)
s <- as.data.frame(s)

# Get the points in our sample in the null space - we can do this by applying
# our decision rule because the rule takes the same form as the hypothesis
# boundary
nulls <- est_post_prob(crit = b_null, post_samples = s, a, c_x, c_y, b_y)[[2]]
nulls <- nulls[!nulls$go,]

names(nulls)[1:2] <- c("x", "y")
nulls$lp <- predict(fit, newdata = nulls)
nulls$p <- 1/(1+exp(-nulls$lp))

nrow(nulls)
nulls <- nulls[1:420000,]
maxes <- NULL
b <- 5000
for(i in 1:(425000/b)){
  ind <- (i-1)*b + 1
  sub <- nulls[ind:(ind+b-1),]
  maxes <- c(maxes, max(sub$p))
}

gev_fit <- fgev(maxes)
gev_fit

# Then our estimate upper limit is location - scale/shape, i.e.

gev_pars <- gev_fit$estimate
gev_pars[1] - gev_pars[2]/gev_pars[3]
```
A Bayesian approach would be nice as it would give a posterior distribution of the upper limit, and we could take an upper quantile estimate from that as our point estimate to be super conservative.

```{r}
library(fitdistcp)
qgev_cp(maxes[!is.na(maxes)], p=c(0.001, 0.9999))
```
