---
title: "vamos - research report"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{vamos - research report}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(vamos)
library(ggplot2)
library(patchwork)
```

## Introduction

This report will compile the theory, evaluation and application of methods for the **vamos** (**va**lue-based testing of **m**ultiple **o**utcome**s**) project. The associated package (which contains this report as a vignette) can be found [here](https://github.com/DTWilson/vamos).

## Problem

Consider a clinical trial comparing two treatments with respect to two normally distributed outcomes with standard deviations $\sigma_i, i=1,2$. Denote the two corresponding treatment effects (i.e., the differences between expected outcomes in the control and experimental treatment groups) and their estimates by $\theta_i$ and $\hat{\theta}_i, i = 1,2$, respectively. Our goal is to develop a procedure for choosing a per-arm sample size $n$ and a critical region $\mathcal{C}$ for a hypothesis test, where we will conclude in favour of the experimental treatment if 

$$
(\hat{z}_1 , \hat{z}_2 ) \in \mathcal{C},
$$
where

$$
\hat{z}_i = \frac{\hat{\theta}_i}{\sqrt{2\sigma_i^2/n}}.
$$

The distribution of $(\hat{z}_1, \hat{z}_2)$ will be bivariate normal with variances of 1 and a covariance of

$$
\begin{aligned}
cov(\hat{z}_1, \hat{z}_2) &= cov\left(\frac{\hat{\theta}_1}{\sqrt{2\sigma_1^2/n}}, \frac{\hat{\theta}_2}{\sqrt{2\sigma_2^2/n}}\right) \\
& = \frac{n}{2 \sigma_1 \sigma_2} cov \left(\hat{\theta_1}, \hat{\theta_2} \right) \\
&= \frac{n}{2 \sigma_1 \sigma_2} \frac{2n}{n^2} \tau\\
&= \frac{\tau}{\sigma_1 \sigma_2}
\end{aligned}
$$
where $\tau$ denotes the covariance between an individual's two outcomes. Since the variances are equal to 1, this covariance is also the correlation.

The conditional distribution of $\hat{z_2}$ given $\hat{z_1}$ is normal with mean

$$
E[\hat{z}_2] + \frac{\tau}{\sigma_1 \sigma_2}(\hat{z}_1 - E[\hat{z}_1])
$$
and variance

$$
1 - \left( \frac{\tau}{\sigma_1 \sigma_2}\right)^2.
$$
  
## Multiple and co-primary endpoints

Two extreme approaches to our problem are based on combining univariate testing procedures for the two outcomes. That is, we choose critical values $c_i, i=1,2$ and then conclude in favour of the experiment treatment when either

- $A = \hat{z}_1 > c_1$ **or** $\hat{z}_2 > c_2$ (the case of multiple primary endpoints), or
- $B = \hat{z}_1 > c_1$ **and** $\hat{z}_2 > c_2$ (the case of co-primary endpoints).

In both cases we choose the design variables $(n, c_1, c_2)$ which minimise $n$ subject to some operating characteristic constraints. For multiple endpoints these are

$$
\begin{align}
\max_{z_1, z_2} & ~ Pr(A ~|~ z_1 \leq 0 \cap z_2 \leq 0) \\
\max_{z_1, z_2} & ~ Pr(A^c  ~|~ z_1 \geq z_1^a \cup z_2 \geq z_2^a),
\end{align}
$$

corresponding to conventional type I and II error rates defined with respect to univariate null (0) and alternative ($z_i^a$) hypotheses. 

For the co-primary case we have instead

$$
\begin{align}
\max_{z_1, z_2} & ~ Pr(B ~|~ z_1 \leq 0 \cup z_2 \leq 0) \\
\max_{z_1, z_2} & ~ Pr(B^c  ~|~ z_1 \geq z_1^a \cap z_2 \geq z_2^a),
\end{align}
$$

again corresponding to type I and II error rates. The hypotheses and critical regions for multiple (i) and co-primary (ii) approaches look like this:

```{r, echo = FALSE}
library(ggplot2)
library(patchwork)

df <- expand.grid(z_1 = c(-5, 7),
                  z_2 = c(-5, 7))

p1 <- ggplot(df, aes(z_1, z_2)) + geom_point(alpha = 0) +
  coord_fixed() + 
  # null
  geom_segment(aes(x = -5, y = 0, xend = 0, yend = 0)) +
  geom_segment(aes(x = 0, y = -5, xend = 0, yend = 0)) + 
  
  # alt
  geom_segment(aes(x = -5, y = 3, xend = 3, yend = 3)) +
  geom_segment(aes(x = 3, y = -5, xend = 3, yend = 3)) +
  
  # crit
  geom_segment(aes(x = -5, y = 2, xend = 2, yend = 2), linetype = 2) +
  geom_segment(aes(x = 2, y = -5, xend = 2, yend = 2), linetype = 2) +
  
  annotate("text", x = -2, y = -2, label = expression(H[0])) +
  annotate("text", x = 5, y = 5, label = expression(H[a])) +
  
  xlab(expression(z[1])) + ylab(expression(z[2])) +
  theme_minimal()

p2 <- ggplot(df, aes(z_1, z_2)) + geom_point(alpha = 0) +
  coord_fixed() + 
  # null
  geom_segment(aes(x = 7, y = 0, xend = 0, yend = 0)) +
  geom_segment(aes(x = 0, y = 7, xend = 0, yend = 0)) + 
  
  # alt
  geom_segment(aes(x = 7, y = 3, xend = 3, yend = 3)) +
  geom_segment(aes(x = 3, y = 7, xend = 3, yend = 3)) +
  
  # crit
  geom_segment(aes(x = 7, y = 2, xend = 2, yend = 2), linetype = 2) +
  geom_segment(aes(x = 2, y = 7, xend = 2, yend = 2), linetype = 2) +
  
  annotate("text", x = -2, y = -2, label = expression(H[0])) +
  annotate("text", x = 5, y = 5, label = expression(H[a])) +
  
  xlab(expression(z[1])) + ylab(expression(z[2])) +
  theme_minimal()  
```

```{r, echo = FALSE, fig.height = 5, fig.width = 7, fig.align = "center"}
p1 + p2 + plot_annotation(tag_levels = 'i')
```

Since co-primary and multiple endpoints are not especially descriptive terms, we will henceforth refer to these as conjunctive and disjunctive tests respectively, following Senn2007.

## Value-based hypotheses

For both the conjunctive and disjunctive approaches, the composite hypotheses imply extreme preference relations between the two outcomes. For example, in the conjunctive case both $(z_1, z_2) = (0, 0)$ and $(z_1, z_2) = (5, 0)$ are considered to be in the null hypothesis, meaning that we wish to constrain the probability of concluding in favour of the experimental treatment to the same level at both points. Whenever there is some kind of trade-off between the two outcomes, we might expect the latter point to be preferred to the former and the permitted probability of a positive result relaxed correspondingly. 

To allow for these kind of preferences we can change the way we set up our hypotheses. We propose to construct these by first choosing lower and upper limits beyond which trade-offs are not considered, denoted $c_1$ and $c_2$. We then define a value contour which goes through the point $(0, c_2)$  use this to construct our null and alternative hypotheses by shrinking or expanding this contour with respect to the point $(c_1, c_2)$.

The types of contours we consider are of the form

$$
z_1 + wz_2 + a z_1 w z_2 = k,
$$

for some constant $k$. The parameter $W$ allows for a relative preference for one endpoint over the other, while the parameter $a$ dictates the nature of the trade-off. With $a = 0$ we will have a linear trade-off; as $a$ increases we will approach the sharp hypothesis illustrated in the conjunctive case above. We will construct hypotheses by first defining the contour which passes through the point $(z_1, z_2) = (0, c_2)$, and then shrink or expand this line with respect to the point $(c_1, c_2)$.

In addition to the parameters $c_1, c_2$ and $a$, the final input parameter we need is denoted $b_2$ and is chosen such that we have the same value at $(0, c_2)$ and $(c_1, b_2)$. Then we can get the weight $w$:

$$
w = c_1/(c_2 - b_2 - ab_2c_1).
$$
Now, since we want $z_2 = 0$ at the point $z_2 = c_2$, we have $k = wc_2$. Our canonical contour is then

$$
z_2 = \frac{w c_2 - z_1} {w + a w z_1}.
$$

To transform this into a hypothesis, we specific some value $b_1$ such that the hypothesis should pass through the point $(b_1, c_2)$. Our hypothesis boundary is then given by mapping the canonical contour to

$$
s \left( \begin{bmatrix} 
z_1 \\ 
z_2 \\ 
\end{bmatrix}  - 
\begin{bmatrix} 
c_1 \\ 
c_2 \\ 
\end{bmatrix}
\right) + 
\begin{bmatrix} 
c_1 \\ 
c_2 \\ 
\end{bmatrix},
$$

where

$$
s = \frac{c_1 - b}{c_1}.
$$

For example, let $c_1 = c_2 = 7$ with $n_y = 0$, $b_{null} = 0$ and $b_{alt} = 2.49$. Consider $a = 0.2, 1, 5, 20$. These produce the following hypotheses:

```{r}
hyp_plot <- function(a, c_x, c_y = c_x, b_y = 0, b_null = 0, b_alt = 2.486475) {
  
  if(a >= 0){
    # co-primary
    x_up <- c_x + 2; y_up <- c_y + 2
    x_lo <- b_null - 2; y_lo <- b_y -2
  } else {
    # mutliple primary
    x_up <- b_alt + 2; y_up <- b_y + 2
    x_lo <- c_x - 2; y_lo <- c_y - 2
  }
  
  # Grid of points for plotting
  df <- expand.grid(x = seq(x_lo, x_up, 0.3),
                 y = seq(y_lo, y_up, 0.3))
  
  # Get weight for value function
  w <- (c_x)/(c_y - b_y - a*c_x*b_y)
  
  # Get the canonical hypothesis passing through (0, c_y)
  df <- data.frame(x = seq(0, c_x, length.out = 100))
  df$y <- (w*c_y - df$x)/(w + a*w*df$x)

  # Null
  s <- (c_x - b_null)/c_x
  df_null <- as.data.frame(t( s*(t(df) - c(c_x, c_y)) + c(c_x, c_y) ))
  
  # Alternative
  s <- (c_x - b_alt)/c_x
  df_alt <- as.data.frame(t( s*(t(df) - c(c_x, c_y)) + c(c_x, c_y) ))
  
  df <- rbind(df_null, df_alt)
  df$h <- rep(c("null", "alt"), each = 100)

  
  ggplot(df, aes(x, y, colour = h)) + geom_line() +
    xlim(c(x_lo, x_up)) + ylim(c(y_lo, y_up)) +
    
    # Straight lines for the null
    geom_segment(aes(x = b_null, xend = b_null, y = (a >= 0)*y_up + (a < 0)*y_lo, yend = c_y, colour = "null")) +
    geom_segment(aes(x = c_x, xend = (a >= 0)*x_up + (a < 0)*x_lo, y =  df_null$y[nrow(df_null)-1], yend = df_null$y[nrow(df_null)-1], colour = "null")) +
      
    # Straight lines for the alternative
    geom_segment(aes(x = b_alt, xend = b_alt, y = (a >= 0)*y_up + (a < 0)*y_lo, yend = c_y, colour = "alt")) +
    geom_segment(aes(x = c_x, xend = (a >= 0)*x_up + (a < 0)*x_lo, y =  df_alt$y[nrow(df_alt)-1], yend = df_alt$y[nrow(df_alt)-1], colour = "alt")) +
    
    scale_color_manual(name = "Hypothesis", breaks = c("null", "alt"), 
                       values = c(2,4), labels = c("N", "A")) +
    xlab(expression(z[1])) + ylab(expression(z[2])) +
    coord_fixed() +
    theme_minimal()
}

p1 <- hyp_plot(0.2, 7); p2 <- hyp_plot(1, 7); p3 <- hyp_plot(5, 7); p4 <- hyp_plot(20, 7)
```

```{r, echo = FALSE, fig.height = 5, fig.width = 7, fig.align = "center"}
(p1 + p2) / (p3 + p4) + plot_layout(guides = "collect")
```

Given we are working on the standardised effect scale, the above assumptions implying a symmetry between the two outcomes may well be reasonable. But we can adjust the relative weighting and hypotheses to specify other more general problems, e.g. when we have a superiority and a non-inferiority outcome with the former considered more important than the latter:

```{r, echo = FALSE, fig.height = 5, fig.width = 7, fig.align = "center"}
hyp_plot(a = 1, c_x = 6, c_y = 9, b_y = -2, b_null = 0, b_alt = 2)
```


We can create similar hypotheses for the case of multiple primary outcomes. For example,

```{r, echo = FALSE, fig.height = 5, fig.width = 7, fig.align = "center"}
hyp_plot(a = -3, c_x = -5, c_y = -5, b_y = -1, b_null = -2, b_alt = 0)
```

A value-based testing procedure can proceed by using the same procedure as for creating hypothesis boundaries to create a critical region. This critical region will be determined by one parameter, which we can choose to ensure type I error rate is controlled everywhere over the null hypothesis boundary. In doing so, we will assume that we are working with large samples such that the covariance matrix of the patient outcomes can be estimated with sufficient precision to let us plug in the estimates when calculating type I error rates. As usual, the power of the trial will rely on an accurate prior estimate of the covariance matrix.

## Results

```{r}
library(mvtnorm)

a <- 1
c <- 7
sigma <- matrix(c(1, 0, 0, 1), ncol = 2)
cor_z <- sigma[1,2]
variance <- sigma[1,1]
alpha_nom <- 0.05
null <- 0.0
alternative <- 0.3

v <- vamos(null, alternative, sigma, alpha_nom, beta_nom=0.2, a=a, c_x=c, c_y=c, n_y=0)
v$n

#ocs_value_cp_wrong(145, null, alternative, cor_z, alpha_nom, variance, a, c_x = c, c_y = c, n_y = 0)
```


## Bayesian

A quick note: if we want to go straight to a Bayesian approach, or to include that as an extension, the following code simulates some bivariate normal data and then does a congugate analysis. The main thing is that it shows a correlation on the outcomes translates to a correlation in the posterior means, and thus supports our general argument still - that uncorrelated endpoints will generally give us lower variance when estimating a joint value or utility.

```{r}
library(MASS)

# Parameters
set.seed(123)
n <- 500
mu <- c(3, 5)
Sigma <- matrix(c(2, 0,
                  0, 2), nrow = 2)

# Simulate data
data <- mvrnorm(n, mu = mu, Sigma = Sigma)

# Plot
plot(data, main = "Simulated Bivariate Normal Data",
     xlab = "X1", ylab = "X2", pch = 19, col = rgb(0, 0, 1, 0.5))

kappa0 <- 1
m0 <- c(0, 0)
nu0 <- 4          # must be > dimension - 1 = 1
S0 <- diag(2)     # prior scale matrix


# Sample mean
xbar <- colMeans(data)

# Scatter matrix
S <- t(data - matrix(xbar, nrow=n, ncol=2, byrow=TRUE)) %*% (data - matrix(xbar, nrow=n, ncol=2, byrow=TRUE))

# Posterior updates
kappa_n <- kappa0 + n
m_n <- (kappa0 * m0 + n * xbar) / kappa_n
nu_n <- nu0 + n

# Scale matrix update
delta <- matrix(xbar - m0, ncol=1)
Sn <- S0 + S + (kappa0 * n / kappa_n) * (delta %*% t(delta))

# Posterior mean
print(m_n)

# Posterior scale matrix
print(Sn)

library(MCMCpack)

# Sample Sigma ~ Inverse-Wishart
Sigma_post <- riwish(nu_n, Sn)

# Sample mu | Sigma
mu_post <- mvrnorm(1, mu = m_n, Sigma = Sigma_post / kappa_n)

print(mu_post)
print(Sigma_post)

mu_posts <- mvrnorm(10^3, mu = m_n, Sigma = Sigma_post / kappa_n)
plot(mu_posts)
cov(mu_posts)
cor(mu_posts)
```
